#! /usr/bin/env python

import argparse
import json
import shutil
import os
import hera.simulations.openFoam.datalayer.ExtractTransformLoadVTK as openfoam
from   hera.simulations.openFoam import DECOMPOSED_CASE, RECONSTRUCTED_CASE


import sys
version = sys.version_info[0]
if version == 3:
    from hera.utils import loadJSON


VTKOutput_folder='VTKOutput'

def load_handler(args):

    """
            This function handles the loading of the OpenFoam runs to the database.
            Converts hdf files results to .parquet, if exist.

        Parameters
        ----------

    """
    if version < 3:
        raise ValueError("Must run the loader with python 3+. ")

    projectName = args.projectName
    pipelineFile = args.pipelineFile
    path=args.casePath

    pipelineFilebaseName = os.path.splitext(os.path.basename(pipelineFile))[0]

    jsondata = loadJSON(pipelineFile)
    pipelines = jsondata["pipelines"]  # Reading the pipelines

    vtkpipe = openfoam.VTKpipeline(pipelineJSON=jsondata, nameOrWorkflowFileOrJSONOrResource=path)
    vtkpipe.loadToProject(projectName=projectName)


def executePipeline_handler(args):

    """
            This function handles the execution of the pipeline.

        Parameters
        ----------

    """
    if version > 2:
        raise ValueError("Must run the pipeline execution with python 2.7 ")

    JSONpath = args.pipelineFile
    JSONbaseName= os.path.splitext(os.path.basename(JSONpath))[0]

    with open(JSONpath) as json_file:
        data = json.load(json_file)

    if args.casePath is None:
        raise ValueError("Must supply the casePath in the command line ")

    caseType = DECOMPOSED_CASE if args.parallelCase else RECONSTRUCTED_CASE

    vtkpipe = openfoam.VTKpipeline(pipelineJSON=data, nameOrWorkflowFileOrJSONOrResource=args.casePath, caseType=caseType, servername=args.serverName)
    vtkpipe.execute("mainReader", tsBlockNum=args.timeStepsInPartition, overwrite=args.overwrite)


def executeLoadPipeline_handler(args):
    """
        Executes the pipeline and then loads it to hera.
    Parameters
    ----------
    args

    Returns
    -------

    """
    executePipeline_handler(args)
    load_handler(args)

if __name__ =="__main__":
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(help='sub-command help')

    ##### Load results to the hera DB
    parser_load = subparsers.add_parser('load', help='load help')
    parser_load.add_argument('projectName',
                             type=str,
                             help='The project name')

    parser_load.add_argument('pipelineFile',
                             type=str,
                             help='[pipeline file name]')

    parser_load.add_argument('-casePath',
                                        dest="casePath",
                                        type=str,
                                        required=True,
                                        help='[path to the case]')

    parser_load.set_defaults(func=load_handler)

    ##### executePipeline parameters
    parser_executePipeline = subparsers.add_parser('executePipeline', help='executePipeline help')
    parser_executePipeline.add_argument('pipelineFile',
                                        type=str,
                                        help='[path to VTK pipeline JSON]')

    parser_executePipeline.add_argument('-casePath',
                                        dest="casePath",
                                        type=str,
                                        required=True,
                                        help='[path to the case]')

    parser_executePipeline.add_argument('-parallelCase',
                                        action='store_true',
                                        dest="parallelCase",
                                        default=True,
                                        help='If exists, reads the decomposed case')

    parser_executePipeline.add_argument('-serverName',
                                        type=str,
                                        dest="serverName",
                                        default=None,
                                        help='If exists, uses the paraview server')

    parser_executePipeline.add_argument('-timeStepsInPartition',
                                        type=int,
                                        dest="timeStepsInPartition",
                                        default=100,
                                        help='The number of time steps in each NetCDF File/ the size of the batch to process')

    parser_executePipeline.add_argument('-overwrite',
                                        action='store_true',
                                        dest = "overwrite",
                                        default=False,
                                        help='If exists, write over the existing files.')

    parser_executePipeline.set_defaults(func=executePipeline_handler)

    ##### Execute and load.
    parser_execload = subparsers.add_parser('executeLoad', help='executes pipeline and then loads it')
    parser_execload.add_argument('projectName',
                             type=str,
                             help='The project name')

    parser_execload.add_argument('pipelineFile',
                             type=str,
                             help='[pipeline file name]')

    parser_execload.add_argument('-casePath',
                                        dest="casePath",
                                        type=str,
                                        required=True,
                                        help='[path to the case]')

    parser_execload.add_argument('-parallelCase',
                                        action='store_true',
                                        dest="parallelCase",
                                        default=True,
                                        help='If exists, reads the decomposed case')

    parser_execload.add_argument('-serverName',
                                        type=str,
                                        dest="serverName",
                                        default=None,
                                        help='If exists, uses the paraview server')

    parser_execload.add_argument('-timeStepsInPartition',
                                        type=int,
                                        dest="timeStepsInPartition",
                                        default=100,
                                        help='The number of time steps in each NetCDF File/ the size of the batch to process')

    parser_execload.add_argument('-overwrite',
                                        action='store_true',
                                        dest = "overwrite",
                                        default=False,
                                        help='If exists, write over the existing files.')

    parser_execload.set_defaults(func=executeLoadPipeline_handler)


    ############################## arg parse
    args = parser.parse_args()
    args.func(args)
