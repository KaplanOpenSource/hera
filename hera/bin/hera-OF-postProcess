#! /usr/bin/env python

import argparse
import json
import shutil
import os

import sys
version = sys.version_info[0]
if version == 2:
    import hera.simulations.openFoam.datalayer.extractVTK as openfoam
    from hera.simulations.openFoam.datalayer.pvOpenFOAMBase import DECOMPOSED_CASE,RECONSTRUCTED_CASE


from hera.simulations.openFoam.datalayer.etlVTK import recurseNode

VTKOutput_folder='VTKOutput'

def load_handler(args):

    """
            This function handles the loading of the OpenFoam runs to the database.
            Converts hdf files results to .parquet, if exist.

        Parameters
        ----------

        args : a positional list pased by the user:
            arguments[0] - the path to the pipeline json file
            arguments[1] - the path to the main directory of the execute resulted files, as the metadata 'datadir' in the json file
            arguments[2] - the name given to the output folder (and to the hdf files if selected)
            arguments[3] - the 'projecName' will be saved to the database


    """

    JSONpath = args.args[0]
    projectName = args.args[1]
    JSONbaseName = os.path.splitext(os.path.basename(JSONpath))[0]

    with open(JSONpath) as json_file:
        jsondata = json.load(json_file)

    metadata = jsondata["metadata"]  # Reading the metadata
    pipelines = jsondata["pipelines"]  # Reading the pipelines
    path=metadata['CaseDirectory']

    for i,node in enumerate(pipelines):
        Tree=[]
        Tree.append(node)
        recurseNode(Tree,node,pipelines[node],metadata,pipelines, path=path,name=VTKOutput_folder,projectName=projectName,JSONName=JSONbaseName)

    #   Delete hdf directory
    if not args.keepHDF:
        print("deleting HDF files")
        shutil.rmtree("%s/%s/hdf" % (path, VTKOutput_folder), ignore_errors=True)
        shutil.rmtree("%s/%s/meta.json" % (path, VTKOutput_folder), ignore_errors=True)


def executePipeline_handler(args):

    """
            This function handles the execution of the pipeline.

        Parameters
        ----------

    """

    JSONpath = args.pipelineFile
    JSONbaseName= os.path.splitext(os.path.basename(JSONpath))[0]

    with open(JSONpath) as json_file:
        data = json.load(json_file)

    if args.casePath is None:
        raise ValueError("Must supply the casePath in the command line ")

    caseType = DECOMPOSED_CASE if args.parallelCase else RECONSTRUCTED_CASE

    vtkpipe = openfoam.VTKpipeline(pipelineJSON=data, casePath=args.casePath, caseType=caseType, servername=args.serverName)
    vtkpipe.execute("mainReader", tsBlockNum=args.timeStepsInPartition, overwrite=args.overwrite)


if __name__ =="__main__":
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(help='sub-command help')

    ##### Load results to the hera DB
    parser_load = subparsers.add_parser('load', help='load help')
    parser_load.add_argument('args',
                             nargs='*',
                             type=str,
                             help='[JSON file name] [ProjectName]')

    parser_load.add_argument('-keepHDF',
                             action='store_true',

                             default=False,
                             help='If keepHDF does not remove the old HDF files')

    parser_load.set_defaults(func=load_handler)

    ##### executePipeline parameters
    parser_executePipeline = subparsers.add_parser('executePipeline', help='executePipeline help')
    parser_executePipeline.add_argument('pipelineFile',
                                        type=str,
                                        help='[path to VTK pipeline JSON]')

    parser_executePipeline.add_argument('-casePath',
                                        dest="casePath",
                                        type=str,
                                        required=True,
                                        help='[path to the case]')

    parser_executePipeline.add_argument('-parallelCase',
                                        action='store_true',
                                        dest="parallelCase",
                                        default=True,
                                        help='If exists, reads the decomposed case')

    parser_executePipeline.add_argument('-serverName',
                                        type=str,
                                        dest="serverName",
                                        default=None,
                                        help='If exists, uses the paraview server')

    parser_executePipeline.add_argument('-timeStepsInPartition',
                                        type=int,
                                        dest="timeStepsInPartition",
                                        default=100,
                                        help='The number of time steps in each NetCDF File/ the size of the batch to process')

    parser_executePipeline.add_argument('-overwrite',
                                        action='store_true',
                                        dest = "overwrite",
                                        default=False,
                                        help='If exists, write over the existing files.')

    parser_executePipeline.set_defaults(func=executePipeline_handler)

    ############################## arg parse
    args = parser.parse_args()
    args.func(args)
