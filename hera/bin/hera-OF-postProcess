#! /usr/bin/env python

import argparse
import json
import os
from   hera.simulations.openFoam import DECOMPOSED_CASE, RECONSTRUCTED_CASE
from hera import toolkitHome,loggingHome

def load_handler(args):

    """
            This function handles the loading of the OpenFoam runs to the database.
            Converts hdf files results to .parquet, if exist.

        Parameters
        ----------

        args.
            projectName : str
                    The name of the project to load to.
            pipelineFile : str
                    The name of the file that contains the pipeline.
            casePath  : str
                    The path to the case, its name, or the workflow file.

    """
    projectName = args.projectName
    pipelineFile = args.pipelineFile
    casePath=args.casePath

    tk = toolkitHome.getToolkit(toolkitName=toolkitHome.SIMULATIONS_OPENFOAM, projectName=projectName)
    pipe = tk.analysis.makeVTKPipeline(nameOrWorkflowFileOrJSONOrResource=casePath, vtkPipeline=pipelineFile)
    pipe.loadToProject()

def executePipeline_handler(args):

    """
            This function handles the execution of the pipeline.

        Parameters
        ----------

    """
    projectName = args.projectName
    pipelineFile = args.pipelineFile
    casePath=args.casePath
    caseType = DECOMPOSED_CASE if args.parallelCase else RECONSTRUCTED_CASE
    tsBlockNum = args.timeStepsInPartition
    overwrite = args.overwrite
    servername = args.serverName
    timeList = args.timeList
    append = args.append

    tk = toolkitHome.getToolkit(toolkitName=toolkitHome.SIMULATIONS_OPENFOAM, projectName=projectName)
    pipe = tk.analysis.makeVTKPipeline(nameOrWorkflowFileOrJSONOrResource=casePath, vtkPipeline=pipelineFile, servername=servername)
    pipe.execute(sourceOrName=None,timeList=timeList, tsBlockNum=tsBlockNum, overwrite=overwrite,append=append)


def executeLoadPipeline_handler(args):
    """
        Executes the pipeline and then loads it to hera.
    Parameters
    ----------
    args

    Returns
    -------

    """
    executePipeline_handler(args)
    load_handler(args)

if __name__ =="__main__":
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(help='sub-command help')

    #loggingHome.addLogger(loggerName="simulations", handlers=['console'], level='DEBUG', propagate=False)

    ##### Load results to the hera DB
    parser_load = subparsers.add_parser('load', help='loading the results to the Project')
    parser_load.add_argument('projectName',
                             type=str,
                             help='The project name')

    parser_load.add_argument('pipelineFile',
                             type=str,
                             help='[pipeline file name]')

    parser_load.add_argument('casePath',help='[path to the case]')

    parser_load.set_defaults(func=load_handler)

    ##### executePipeline parameters
    parser_executePipeline = subparsers.add_parser('executePipeline', help='executing the pipeline filters (writing to parquet/netcdf files)')
    parser_executePipeline.add_argument('projectName',
                             type=str,
                             help='The project name')

    parser_executePipeline.add_argument('pipelineFile',
                             type=str,
                             help='[pipeline file name]')

    parser_executePipeline.add_argument('casePath',help='[path to the case]')

    parser_executePipeline.add_argument('--parallelCase',
                                        action='store_true',
                                        dest="parallelCase",
                                        default=True,
                                        help='If exists, reads the decomposed case')

    parser_executePipeline.add_argument('--serverName',
                                        type=str,
                                        dest="serverName",
                                        default=None,
                                        help='If exists, uses the paraview server')

    parser_executePipeline.add_argument('--timeStepsInPartition',
                                        type=int,
                                        dest="timeStepsInPartition",
                                        default=50,
                                        help='The number of time steps in each NetCDF File/ the size of the batch to process')

    parser_executePipeline.add_argument('--overwrite',
                                        action='store_true',
                                        dest = "overwrite",
                                        default=False,
                                        help='Write over the existing files.')

    parser_executePipeline.add_argument('--append',
                                        action='store_true',
                                        dest = "append",
                                        default=False,
                                        help='Append to the existing files.')

    parser_executePipeline.set_defaults(func=executePipeline_handler)

    ##### Execute and load.
    parser_execload = subparsers.add_parser('executeLoad', help='executes pipeline and then loads it')
    parser_execload.add_argument('projectName',
                             type=str,
                             help='The project name')

    parser_execload.add_argument('pipelineFile',
                             type=str,
                             help='[pipeline file name]')

    parser_execload.add_argument('casePath',help='[path to the case]')

    parser_execload.add_argument('--parallelCase',
                                action='store_true',
                                dest="parallelCase",
                                default=True,
                                help='If exists, reads the decomposed case')

    parser_execload.add_argument('--serverName',
                                type=str,
                                dest="serverName",
                                default=None,
                                help='If exists, uses the paraview server')

    parser_execload.add_argument('--timeStepsInPartition',
                                type=int,
                                dest="timeStepsInPartition",
                                default=50,
                                help='The number of time steps in each NetCDF File/ the size of the batch to process')

    parser_execload.add_argument('--overwrite',
                                    action='store_true',
                                    dest = "overwrite",
                                    default=False,
                                    help='Write over the existing files.')

    parser_execload.add_argument('--append',
                                        action='store_true',
                                        dest = "append",
                                        default=False,
                                        help='Append to the existing files.')


    parser_execload.set_defaults(func=executeLoadPipeline_handler)


    ############################## arg parse
    args = parser.parse_args()
    args.func(args)
