#! /usr/bin/env python

import argparse
import json
import sys
import shutil
import os

version = sys.version_info[0]
from hera.simulations.openfoam.preprocess.etlVTK import recurseNode
from hera.simulations.openfoam.preprocess import RECONSTRUCTED_CASE,DECOMPOSED_CASE

if version == 2:
    from hera import openfoam

VTKOutput_folder='VTKOutput'

def load_handler(args):

    """
            This function handles the loading of the OpenFoam runs to the database.
            Converts hdf files results to .parquet, if exist.



        Parameters
        ----------

        args : a positional list pased by the user:
            arguments[0] - the path to the pipeline json file
            arguments[1] - the path to the main directory of the execute resulted files, as the metadata 'datadir' in the json file
            arguments[2] - the name given to the output folder (and to the hdf files if selected)
            arguments[3] - the 'projecName' will be saved to the database


    """

    JSONpath = args.args[0]
    projectName = args.args[1]
    JSONbaseName = os.path.splitext(os.path.basename(JSONpath))[0]

    with open(JSONpath) as json_file:
        jsondata = json.load(json_file)

    metadata = jsondata["metadata"]  # Reading the metadata
    pipelines = jsondata["pipelines"]  # Reading the pipelines
    path=metadata['CaseDirectory']


    for i,node in enumerate(pipelines):
        Tree=[]
        Tree.append(node)
        recurseNode(Tree,node,pipelines[node],metadata,pipelines, path=path,name=VTKOutput_folder,projectName=projectName,JSONName=JSONbaseName)

    #   Delete hdf directory
    if not args.keepHDF:
        print("deleting HDF files")
        shutil.rmtree("%s/%s/hdf" % (path, VTKOutput_folder), ignore_errors=True)
        shutil.rmtree("%s/%s/meta.json" % (path, VTKOutput_folder), ignore_errors=True)


def executePipeline_handler(args):

    """
            This function handles the execution of the pipeline.

        Parameters
        ----------

        args : a positional list pased by the user:
            arguments[0] - the path to the pipeline json file
            arguments[1] - the name which will be given to the output folder and to the hdf files
            arguments[2] - the path to the directory of the openFOAM project contains the results
            arguments[3] - optional. 'Decomposed Case' for parallel cases or 'Reconstructed Case' for single processor cases. default is 'Decomposed Case'.
            arguments[4] - optional. a connection string to paraview server.default is None (work locally).
            arguments[5] - optional. the number of time steps will be saved to single file. default is 100

    """

    JSONpath = args.args[0]
    JSONbaseName= os.path.splitext(os.path.basename(JSONpath))[0]

    with open(JSONpath) as json_file:
        data = json.load(json_file)

    casePath = data["metadata"].get("CaseDirectory", "None")
    caseType = DECOMPOSED_CASE if args.parallelCase else RECONSTRUCTED_CASE

    vtkpipe = openfoam.VTKpipeline(name=VTKOutput_folder, pipelineJSON=data, casePath=casePath, caseType=caseType, servername=args.serverName)
    vtkpipe.execute("mainReader", tsBlockNum=args.timeStepsInPartition, JSONName=JSONbaseName, overWrite=args.overwrite)


if __name__ =="__main__":
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(help='sub-command help')

    parser_load = subparsers.add_parser('load', help='load help')
    parser_executePipeline = subparsers.add_parser('executePipeline', help='executePipeline help')

    ##### Load parameters
    parser_load.add_argument('args',
                             nargs='*',
                             type=str,
                             help='[JSON file name] [ProjectName]')

    parser_load.add_argument('-keepHDF',
                             action='store_true',
                             default=False,
                             help='If keepHDF does not remove the old HDF files')

    parser_load.set_defaults(func=load_handler)

    ##### executePipeline parameters
    parser_executePipeline.add_argument('args',
                                        nargs='*',
                                        type=str,
                                        help='[path to JSON]')

    parser_executePipeline.add_argument('-parallelCase',
                                        action='store_true',
                                        default=False,
                                        help='If exists, reads the decomposed case')

    parser_executePipeline.add_argument('-serverName',
                                        type=str,
                                        default=None,
                                        help='If exists, uses the paraview server')

    parser_executePipeline.add_argument('-timeStepsInPartition',
                                        type=int,
                                        default=100,
                                        help='The number of time steps in each HDF/NetCDF File')

    parser_executePipeline.add_argument('-overwrite',
                                        action='store_true',
                                        default=False,
                                        help='If exists, write over the existing files.')

    parser_executePipeline.set_defaults(func=executePipeline_handler)

    ############################## arg parse
    args = parser.parse_args()
    args.func(args)
